{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016a768",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.models import Word2Vec\n",
    "from scipy import spatial\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3225e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'Training_data.csv')\n",
    "df_test = pd.read_csv(r'Test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146c80c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "summary_test1 = []\n",
    "for col in df_test:\n",
    "    if col == 'text1':\n",
    "        for i, row_value in df_test[col].iteritems():\n",
    "            final=\"\"\n",
    "            try:\n",
    "                text=df_test[col][i]\n",
    "                sentences=sent_tokenize(text)\n",
    "                sentences_clean=[re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentences]\n",
    "                stop_words = stopwords.words('english')\n",
    "                sentence_tokens=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]\n",
    "                w2v=Word2Vec(sentence_tokens,vector_size=1,min_count=1)\n",
    "                sentence_embeddings=[[w2v.wv[word][0] for word in words] for words in sentence_tokens]\n",
    "                max_len=max([len(tokens) for tokens in sentence_tokens])\n",
    "                sentence_embeddings=[np.pad(embedding,(0,max_len-len(embedding)),'constant') for embedding in sentence_embeddings]\n",
    "                similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\n",
    "                for i,row_embedding in enumerate(sentence_embeddings):\n",
    "                    for j,column_embedding in enumerate(sentence_embeddings):\n",
    "                        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)\n",
    "                nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "                scores = nx.pagerank_numpy(nx_graph)\n",
    "                top_sentence={sentence:scores[index] for index,sentence in enumerate(sentences)}\n",
    "                top=dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "                for sent in sentences:\n",
    "                    if sent in top.keys():\n",
    "                        final = final + sent\n",
    "                summary_test1.append(final)\n",
    "            except TypeError:\n",
    "                summary_test1.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200eff59",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_train1 = []\n",
    "for col in df_train:\n",
    "    if col == 'text1':\n",
    "        for i, row_value in df_train[col].iteritems():\n",
    "            final=\"\"\n",
    "            try:\n",
    "                text=df_train[col][i]\n",
    "                sentences=sent_tokenize(text)\n",
    "                sentences_clean=[re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentences]\n",
    "                stop_words = stopwords.words('english')\n",
    "                sentence_tokens=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]\n",
    "                w2v=Word2Vec(sentence_tokens,vector_size=1,min_count=1)\n",
    "                sentence_embeddings=[[w2v.wv[word][0] for word in words] for words in sentence_tokens]\n",
    "                max_len=max([len(tokens) for tokens in sentence_tokens])\n",
    "                sentence_embeddings=[np.pad(embedding,(0,max_len-len(embedding)),'constant') for embedding in sentence_embeddings]\n",
    "                similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\n",
    "                for i,row_embedding in enumerate(sentence_embeddings):\n",
    "                    for j,column_embedding in enumerate(sentence_embeddings):\n",
    "                        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)\n",
    "                nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "                scores = nx.pagerank_numpy(nx_graph)\n",
    "                top_sentence={sentence:scores[index] for index,sentence in enumerate(sentences)}\n",
    "                top=dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "                for sent in sentences:\n",
    "                    if sent in top.keys():\n",
    "                        final = final + sent\n",
    "                summary_train1.append(final)\n",
    "            except TypeError:\n",
    "                summary_train1.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8b9937",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_test2 = []\n",
    "for col in df_test:\n",
    "    if col == 'text2':\n",
    "        for i, row_value in df_test[col].iteritems():\n",
    "            final=\"\"\n",
    "            try:\n",
    "                text=df_test[col][i]\n",
    "                sentences=sent_tokenize(text)\n",
    "                sentences_clean=[re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentences]\n",
    "                stop_words = stopwords.words('english')\n",
    "                sentence_tokens=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]\n",
    "                w2v=Word2Vec(sentence_tokens,vector_size=1,min_count=1)\n",
    "                sentence_embeddings=[[w2v.wv[word][0] for word in words] for words in sentence_tokens]\n",
    "                max_len=max([len(tokens) for tokens in sentence_tokens])\n",
    "                sentence_embeddings=[np.pad(embedding,(0,max_len-len(embedding)),'constant') for embedding in sentence_embeddings]\n",
    "                similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\n",
    "                for i,row_embedding in enumerate(sentence_embeddings):\n",
    "                    for j,column_embedding in enumerate(sentence_embeddings):\n",
    "                        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)\n",
    "                nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "                scores = nx.pagerank_numpy(nx_graph)\n",
    "                top_sentence={sentence:scores[index] for index,sentence in enumerate(sentences)}\n",
    "                top=dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "                for sent in sentences:\n",
    "                    if sent in top.keys():\n",
    "                        final = final + sent\n",
    "                summary_test2.append(final)\n",
    "            except TypeError:\n",
    "                summary_test2.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328268a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_train2 = []\n",
    "for col in df_test:\n",
    "    if col == 'text2':\n",
    "        for i, row_value in df_test[col].iteritems():\n",
    "            final=\"\"\n",
    "            try:\n",
    "                text=df_test[col][i]\n",
    "                sentences=sent_tokenize(text)\n",
    "                sentences_clean=[re.sub(r'[^\\w\\s]','',sentence.lower()) for sentence in sentences]\n",
    "                stop_words = stopwords.words('english')\n",
    "                sentence_tokens=[[words for words in sentence.split(' ') if words not in stop_words] for sentence in sentences_clean]\n",
    "                w2v=Word2Vec(sentence_tokens,vector_size=1,min_count=1)\n",
    "                sentence_embeddings=[[w2v.wv[word][0] for word in words] for words in sentence_tokens]\n",
    "                max_len=max([len(tokens) for tokens in sentence_tokens])\n",
    "                sentence_embeddings=[np.pad(embedding,(0,max_len-len(embedding)),'constant') for embedding in sentence_embeddings]\n",
    "                similarity_matrix = np.zeros([len(sentence_tokens), len(sentence_tokens)])\n",
    "                for i,row_embedding in enumerate(sentence_embeddings):\n",
    "                    for j,column_embedding in enumerate(sentence_embeddings):\n",
    "                        similarity_matrix[i][j]=1-spatial.distance.cosine(row_embedding,column_embedding)\n",
    "                nx_graph = nx.from_numpy_array(similarity_matrix)\n",
    "                scores = nx.pagerank_numpy(nx_graph)\n",
    "                top_sentence={sentence:scores[index] for index,sentence in enumerate(sentences)}\n",
    "                top=dict(sorted(top_sentence.items(), key=lambda x: x[1], reverse=True)[:5])\n",
    "                for sent in sentences:\n",
    "                    if sent in top.keys():\n",
    "                        final = final + sent\n",
    "                summary_train2.append(final)\n",
    "            except TypeError:\n",
    "                summary_train2.append(final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6961383",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test[\"text_summary1\"] = summary_test1\n",
    "df_test[\"text_summary2\"] = summary_test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28af9ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"text_summary1\"] = summary_train1\n",
    "df_train[\"text_summary2\"] = summary_train2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd506a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.to_csv('Test_data.csv', encoding='utf-8')\n",
    "df_train.to_csv('Training_data.csv', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3008d3f4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
