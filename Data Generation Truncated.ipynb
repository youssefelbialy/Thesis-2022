{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80e614bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "import torch\n",
    "import re\n",
    "from transformers import XLMRobertaTokenizer, XLMRobertaForSequenceClassification\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70960eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv(r'C:\\Users\\youss\\Downloads\\Train_data.csv')\n",
    "df_test = pd.read_csv(r'C:\\Users\\youss\\Downloads\\Test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848ccef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_TYPE = 'xlm-roberta-large'\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(MODEL_TYPE)\n",
    "MAX_LEN = 512\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a37e6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "        encoded_dict = tokenizer.encode_plus(\n",
    "            str(cell1), str(cell2), \n",
    "            add_special_tokens = True,\n",
    "            max_length = MAX_LEN,     \n",
    "            pad_to_max_length = True,\n",
    "            return_attention_mask = True,   \n",
    "            return_tensors = 'pt' # return pytorch tensors\n",
    "       )\n",
    "        padded_token_list = encoded_dict['input_ids'][0]\n",
    "        att_mask = encoded_dict['attention_mask'][0]\n",
    "\n",
    "        sample = {\"Padded Token List\": padded_token_list, \"Attention Mask\": att_mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c75e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "truncated_text1 = []\n",
    "truncated_text2 = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c39c2216",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncate1 (text, length):\n",
    "    if(length1 > 256):\n",
    "        text_ids = tokenizer.encode(text)\n",
    "        text_trunc_ids = text_ids[-28:]\n",
    "        text_trunc_tokens = tokenizer.convert_ids_to_tokens(text_trunc_ids)\n",
    "        text_trunc_back_sent = ''.join([x.replace('▁', ' ') for x in text_trunc_tokens])[:-1]\n",
    "        text_trunc_ids1 = text_ids[:100]\n",
    "        text_trunc_tokens1 = tokenizer.convert_ids_to_tokens(text_trunc_ids1)\n",
    "        text_trunc_back_sent1 = ''.join([x.replace('▁', ' ') for x in text_trunc_tokens1])[:-1]\n",
    "        full_text = text_trunc_back_sent1 + text_trunc_back_sent\n",
    "        truncated_text1.append(full_text)\n",
    "    else:\n",
    "        truncated_text1.append(text)\n",
    "def truncate2 (text, length):\n",
    "    if(length2 > 256):\n",
    "        text_ids = tokenizer.encode(text)\n",
    "        text_trunc_ids = text_ids[-28:]\n",
    "        text_trunc_tokens = tokenizer.convert_ids_to_tokens(text_trunc_ids)\n",
    "        text_trunc_back_sent = ''.join([x.replace('▁', ' ') for x in text_trunc_tokens])[:-1]\n",
    "        text_trunc_ids1 = text_ids[:100]\n",
    "        text_trunc_tokens1 = tokenizer.convert_ids_to_tokens(text_trunc_ids1)\n",
    "        text_trunc_back_sent1 = ''.join([x.replace('▁', ' ') for x in text_trunc_tokens1])[:-1]\n",
    "        full_text = text_trunc_back_sent1 + text_trunc_back_sent\n",
    "        truncated_text2.append(full_text)\n",
    "    else:\n",
    "        truncated_text2.append(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5603c61d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in df_train:\n",
    "    if col == 'text1':\n",
    "        for i, row_value in df_train[col].iteritems():\n",
    "            text1 = str(row_value)\n",
    "            text1 = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text1)\n",
    "            length1 = len(re.findall(r'\\w+', text1))\n",
    "            truncate1(text1, length1)\n",
    "    if col == 'text2':\n",
    "        for i, row_value in df_train[col].iteritems():\n",
    "            text2 = str(row_value)\n",
    "            text2 = re.sub(r'''(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\".,<>?«»“”‘’]))''', \" \", text2)\n",
    "            length2 = len(re.findall(r'\\w+', text2))\n",
    "            truncate2(text2, length2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cddff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train['text1_truncate'] = truncated_text1\n",
    "df_train['text2_truncate'] = truncated_text2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd6371d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.to_csv('Training_data.csv', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
